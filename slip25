19.	Create a two layered neural network with relu and sigmoid activation function. 


import numpy as np
# Set random seed for reproducibility
np.random.seed(42)
# Input data: 3 examples with 2 features each
X = np.array([[0.1, 0.2, 0.3],
              [0.4, 0.5, 0.6]])
# Network architecture
n_input = 2   # Number of input features
n_hidden = 4  # Number of neurons in the hidden layer
n_output = 1  # Output size (binary classification)
# Initialize weights and biases
W1 = np.random.randn(n_hidden, n_input) * 0.01  
b1 = np.zeros((n_hidden, 1))                    
W2 = np.random.randn(n_output, n_hidden) * 0.01 
b2 = np.zeros((n_output, 1))                    
# Forward propagation (without separate functions)
# Layer 1: Hidden Layer
Z1 = np.dot(W1, X) + b1  # Linear transformation
A1 = np.maximum(0, Z1)   # ReLU activation
# Layer 2: Output Layer
Z2 = np.dot(W2, A1) + b2  # Linear transformation
A2 = 1 / (1 + np.exp(-Z2))  # Sigmoid activation
# Output
print("Network Output:\n", A2)
